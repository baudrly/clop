{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1701419409079
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import CLIPModel, AutoModel, CLIPProcessor\n",
        "from preprocessing import fcgr, protein_to_dna\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "#from CLIP.clip import clip\n",
        "#from CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "from clip import clip\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class image_title_dataset(Dataset):\n",
        "    def __init__(self, processor, images, labels, max_len):\n",
        "        # Initialize image paths and corresponding texts\n",
        "        self.images = images\n",
        "        self.max_len = max_len\n",
        "        # Tokenize text using CLIP's tokenizer\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Preprocess image using CLIP's preprocessing function\n",
        "        labels_short = self.labels[idx:idx + 1].tolist()[:][:self.max_len]\n",
        "\n",
        "        tokens = clip.tokenize(self.labels[idx:idx + 1].tolist()[:50], context_length = 256,truncate=True)[:1, :self.max_len]\n",
        "\n",
        "        if tokens.size()[1] < self.max_len:\n",
        "            tokens = torch.cat([tokens,\n",
        "                       torch.zeros(size=(1, self.max_len - tokens.size()[1]))], dim=1).type(\n",
        "                torch.int)\n",
        "        #attmask = torch.zeros()\n",
        "        #pixel_values = torch.transforms\n",
        "\n",
        "        inputs = self.processor(text=labels_short, images=self.images[idx:idx+1], return_tensors=\"pt\", padding=True)\n",
        "        inputs['input_ids'] = tokens\n",
        "        if inputs['attention_mask'].size()[1] > self.max_len:\n",
        "            inputs['attention_mask'] = inputs['attention_mask'][:1, :self.max_len]\n",
        "        inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.zeros(size=(1, len(inputs['input_ids'][0]) - len(inputs['attention_mask'][0])))], dim=1).type(torch.int)\n",
        "\n",
        "        #inputs['input_ids'] = torch.cat([inputs['input_ids'], torch.Tensor([[0] * (self.max_len - len(inputs['input_ids'][0]))])], dim=1)\n",
        "        #inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.Tensor([[0] * (self.max_len - len(inputs['attention_mask'][0]))])], dim=1)\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        if p.requires_grad:\n",
        "            p.grad.data = p.grad.data.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_training(model,loader_test, criterion):\n",
        "    loss_all = []\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, inputs in enumerate(loader_test):\n",
        "            outputs = model(input_ids=torch.Tensor(inputs['input_ids']).type(torch.int),attention_mask=torch.Tensor(inputs['attention_mask']).squeeze(1), pixel_values=torch.Tensor(inputs['pixel_values']).squeeze(1))\n",
        "            \n",
        "            logits_i = outputs.logits_per_image\n",
        "            logits_t = outputs.logits_per_text\n",
        "            #probs = logits.softmax(dim=1)\n",
        "\n",
        "            labels = torch.arange(0, logits_i.shape[0])\n",
        "            loss_i = criterion(logits_i, labels)\n",
        "            loss_t = criterion(logits_t, labels)\n",
        "\n",
        "            loss = (loss_i + loss_t)/2\n",
        "            loss_all.append(loss.item())\n",
        "\n",
        "    return np.mean(loss_all)\n",
        "        \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, processor, images, labels, epochs=1):\n",
        "    #tokenizer = get_or_build_tokenizer(labels)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #for img in images:\n",
        "\n",
        "    #labeltokens = clip.tokenize(labels.tolist(), 128)\n",
        "\n",
        "    #labeltokens = np.array([np.array(tokenizer.encode(label).ids) for label in labels])\n",
        "\n",
        "    #inputs = processor(text=labeltokens, images=images, return_tensors='pt', padding=True)\n",
        "    #inputs = processor(text=torch.Tensor(labeltokens[:1]), images=torch.permute(images[:1], dims=(0,2,3,1)) )\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = Adam(params, lr=1e-5, weight_decay=0.0001)\n",
        "    \n",
        "    n=len(labels)\n",
        "    assert n == images.shape[0]\n",
        "    \n",
        "    images_train = images[:int(n*0.9)]\n",
        "    images_test = images[int(n*0.9):]\n",
        "    labels_train = labels[:int(n*0.9)]\n",
        "    labels_test = labels[int(n*0.9):]\n",
        "\n",
        "    \n",
        "    dataset_train = image_title_dataset(processor, images_train, labels_train, 77) #instead of 77\n",
        "    loader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "\n",
        "    dataset_test = image_title_dataset(processor, images_test, labels_test, 77) #instead of 77\n",
        "    loader_test = DataLoader(dataset_test, batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_epoch = []\n",
        "        print(f\"Epoch {epoch+1}/{epochs}--------------------------------\")\n",
        "        for i, inputs in enumerate(loader_train):\n",
        "            \n",
        "        #inputs = processor(text=labels[:].tolist(), images=images[:], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=torch.Tensor(inputs['input_ids']).type(torch.int),\n",
        "                attention_mask=torch.Tensor(inputs['attention_mask']).squeeze(1), pixel_values=torch.Tensor(inputs['pixel_values']).squeeze(1))\n",
        "\n",
        "            logits_i = outputs.logits_per_image\n",
        "            logits_t = outputs.logits_per_text\n",
        "            #probs = logits.softmax(dim=1)\n",
        "\n",
        "            labels = torch.arange(0, logits_i.shape[0])\n",
        "            loss_i = criterion(logits_i, labels)\n",
        "            loss_t = criterion(logits_t, labels)\n",
        "\n",
        "            loss = (loss_i + loss_t)/2\n",
        "            loss.backward()\n",
        "\n",
        "            if device == 'cpu':\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                convert_models_to_fp32(model)\n",
        "                optimizer.step()\n",
        "                clip.model.convert_weights(model)\n",
        "\n",
        "            print(f\"{loss.item()}... batch {i+1}/{len(loader_train)}\")\n",
        "            loss_epoch.append(loss.item())\n",
        "    \n",
        "            if i%10==0:\n",
        "                loss_epoch_test = evaluate_training(model,loader_test,criterion)\n",
        "                print(f\"Current eval loss after batch {i+1}/{len(loader_train)} epoch={epoch} = {loss_epoch_test}\")\n",
        "\n",
        "        loss_epoch = np.mean(loss_epoch)\n",
        "        print(f\"Loss epoch {epoch} = {loss_epoch}\")\n",
        "        \n",
        "        loss_epoch_test = evaluate_training(model,loader_test,criterion)\n",
        "        print(f\"Final eval loss epoch={epoch} = {loss_epoch_test}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model, processor = clip.load(\"ViT-B/32\",device=device,jit=True) #Must set jit=False for training\n",
        "\"\"\"\n",
        "    model : torch.nn.Module\n",
        "        The CLIP model\n",
        "\n",
        "\n",
        "    preprocess : Callable[[PIL.Image], torch.Tensor]\n",
        "        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in model.state_dict().keys(): print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nsamples = 10000\n",
        "labelcol = \"Protein names\"\n",
        "inputcol = \"Sequence\"\n",
        "labels = pd.read_csv(r\"labels.csv\")[labelcol][:nsamples]\n",
        "inputs = pd.read_csv(r\"sequences.csv\")[inputcol][:nsamples]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = np.array([fcgr(seq, k=7) for seq in inputs])\n",
        "images = np.array([img/np.sum(img) for img in images])\n",
        "images = torch.Tensor(images).unsqueeze(1).repeat(1, 3, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(model, processor, images, labels, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "model_path = os.getcwd()\n",
        "\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
        "\n",
        "\n",
        "model_name = f\"model_clop_({timestamp}).ckpt\"\n",
        "torch.save(model.state_dict(), os.path.join(model_path, model_name) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.load(os.path.join(model_path, \"model_clop_(2023_12_01-12_57_49).ckpt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ****************************************************"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = image_title_dataset(processor, images[:1000], labels[:1000], max_len=77)\n",
        "loader = DataLoader(data, batch_size=1000, shuffle=True)\n",
        "len(loader)\n",
        "inputs = list(loader)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = model(input_ids=torch.Tensor(inputs['input_ids']).type(torch.int),\n",
        "                attention_mask=torch.Tensor(inputs['attention_mask']).squeeze(1), \n",
        "                pixel_values=torch.Tensor(inputs['pixel_values']).squeeze(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_embeddings = outputs.text_embeds\n",
        "images_embeddings = outputs.image_embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seqs =[\"ACGTCGTAGCTAGCTAGCTAGCTAGCTAGTCAGCTA\"]\n",
        "imgs = np.array([fcgr(seq, k=7) for seq in seqs])\n",
        "imgs = np.array([img/np.sum(img) for img in imgs])\n",
        "imgs = torch.Tensor(imgs).unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "lbls = pd.Series([\"homo sapiens\"])\n",
        "\n",
        "data = image_title_dataset(processor, imgs, lbls, max_len=77)\n",
        "loader = DataLoader(data, batch_size=1, shuffle=True)\n",
        "outputs = model(input_ids=torch.Tensor(inputs['input_ids']).type(torch.int),\n",
        "                attention_mask=torch.Tensor(inputs['attention_mask']).squeeze(1), \n",
        "                pixel_values=torch.Tensor(inputs['pixel_values']).squeeze(1))\n",
        "\n",
        "lbl_embedding = outputs.text_embeds\n",
        "img_embedding = outputs.image_embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_embeddings.shape\n",
        "lbl_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.multiply(lbl_embedding, labels_embeddings).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "image_encoded = model.vision_model.encoder(inputs['pixel_values'])\n",
        "image_encoded"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "swiss-androsace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
